{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L-layer deep nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJQnVxCeTHEYBPa7OGP1Vk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekyadav21/L-layer-neural-network-right-from-scratch-using-numpy/blob/python-implementation/L_layer_deep_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwoqlV-2YuJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # number of layers in the networ\n",
        "        \n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*(np.sqrt(1/layer_dims[l]))\n",
        "        parameters['b' + str(l)] = np.zeros(shape=(layer_dims[l],1))        \n",
        "    return parameters\n",
        "\n",
        "def sigmoid(Z):\n",
        "  activation = 1/(1+np.exp(-Z))\n",
        "  return activation,Z\n",
        "\n",
        "def relu(Z):\n",
        "  activation = np.maximum(0,Z)\n",
        "  return activation,Z\n",
        "\n",
        "\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z =np.dot(W,A) + b\n",
        "\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2   # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "\n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n",
        "        caches.append(cache)\n",
        "        \n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "\n",
        "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation='sigmoid')\n",
        "    caches.append(cache)           \n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "\n",
        "    cost = -1*(np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)))/m\n",
        "\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    return cost\n",
        "\n",
        "\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "\n",
        "    dW = (np.dot(dZ,A_prev.T))/m\n",
        "    db = (np.sum(dZ,axis=1,keepdims=1))/m\n",
        "    dA_prev =np.dot(W.T,dZ)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def relu_backward(dA,activation_cache):\n",
        "  temp = (activation_cache > 0).astype(int)\n",
        "  dZ = dA * temp\n",
        "  return dZ\n",
        "\n",
        "\n",
        "def sigmoid_backward(dA, activation_cache):\n",
        "  temp=sigmoid(activation_cache)\n",
        "  temp[0]\n",
        "  dZ= dA*(temp[0] *(1-temp[0]))\n",
        "  return dZ\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "\n",
        "        dZ = sigmoid_backward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    return grads\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP-L24FlbtBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CONSTANTS ###\n",
        "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPNaexXMbuIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. \n",
        "    \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "       \n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        \n",
        "        # Compute cost.\n",
        "        \n",
        "        cost = compute_cost(AL, Y)\n",
        "        \n",
        "    \n",
        "        # Backward propagation.\n",
        "        \n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        \n",
        " \n",
        "        # Update parameters.\n",
        "        \n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        \n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCkvxF8JmvvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IllChChm3Nv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e9e58d9-edf5-47bd-b968-91be1ad9381d"
      },
      "source": [
        "with h5py.File('/content/train_catvnoncat.h5','r') as hdf:\n",
        "  ls=list(hdf.keys())\n",
        "  print(\"datasets are\",ls)\n",
        "  train_org_x=hdf.get('train_set_x')\n",
        "  train_org_x=np.array(train_org_x)\n",
        "  train_org_y=hdf.get('train_set_y')\n",
        "  train_org_y=np.array(train_org_y)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets are ['list_classes', 'train_set_x', 'train_set_y']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Dd0U-louLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d559f29b-bfd7-42b1-90f2-55a92b116171"
      },
      "source": [
        "train_org_y"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PH3bDNQocTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "373c6450-47dd-49de-f303-95d4292fd4dd"
      },
      "source": [
        "with h5py.File('/content/test_catvnoncat.h5','r') as hdf:\n",
        "  ls=list(hdf.keys())\n",
        "  print(\"datasets are\",ls)\n",
        "  test_org_x=hdf.get('test_set_x')\n",
        "  test_org_x=np.array(test_org_x)\n",
        "  test_org_y=hdf.get('test_set_y')\n",
        "  test_org_y=np.array(test_org_y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets are ['list_classes', 'test_set_x', 'test_set_y']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBnSiorCpaT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape the training and test examples\n",
        "train_x_flatten = train_org_x.reshape(train_org_x.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_org_x.reshape(test_org_x.shape[0], -1).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrjsCS_Cpogt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvQ_bOVHrCGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y=train_org_y.reshape(1,train_org_y.shape[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOjRT7TLyt-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_y=test_org_y.reshape(1,test_org_y.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6tJcQIiqK7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "58db914e-640e-4ed6-e88b-a2abeac1018f"
      },
      "source": [
        "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 9.472371\n",
            "Cost after iteration 100: 0.600182\n",
            "Cost after iteration 200: 0.556037\n",
            "Cost after iteration 300: 0.522362\n",
            "Cost after iteration 400: 0.496378\n",
            "Cost after iteration 500: 0.467151\n",
            "Cost after iteration 600: 0.443936\n",
            "Cost after iteration 700: 0.413885\n",
            "Cost after iteration 800: 0.391009\n",
            "Cost after iteration 900: 0.369075\n",
            "Cost after iteration 1000: 0.351792\n",
            "Cost after iteration 1100: 0.331722\n",
            "Cost after iteration 1200: 0.312702\n",
            "Cost after iteration 1300: 0.294917\n",
            "Cost after iteration 1400: 0.278593\n",
            "Cost after iteration 1500: 0.263580\n",
            "Cost after iteration 1600: 0.248117\n",
            "Cost after iteration 1700: 0.233447\n",
            "Cost after iteration 1800: 0.215339\n",
            "Cost after iteration 1900: 0.199836\n",
            "Cost after iteration 2000: 0.187088\n",
            "Cost after iteration 2100: 0.173308\n",
            "Cost after iteration 2200: 0.158540\n",
            "Cost after iteration 2300: 0.146740\n",
            "Cost after iteration 2400: 0.136339\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}